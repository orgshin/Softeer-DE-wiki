## 리뷰

### 강의
- Catalyst Optimizer는 생각 이상으로 logical optimizer를 많이 해준다. ( Rule이 모여있는게 Catalog )
- Whole-Stage : 하나의 Stage를 통으로 하나의 function으로 만듬 >> Shuffle이 안 일어나는 최대의 공간에서 최고의 효율을 내기 위해서
- 코스트가 낮은걸 선택하는데 데이터 특성에 따라 코스트가 달라지기 때문에 Physical Plans가 여러개임 ( Cost model이 없으면 optimizer 할 방법이 없다 )
- RDBMS에서는 left join, Spark에서는 해시 테이블을 가지고 필터를 침 / 테이블에서 특정 조건에 대해 뽑는다고 치면 컬럼을 추가하면 훨씬 싸다 > RDBMS에서는 컬럼을 추가할 생각을 못했지만, 여기선 다름
- 파티션 pruning의 장점은 처리 할 데이터의 양이 작아짐
- AQE : 실행하면서 Physical plan을 바꿈
- 알고봤더니 데이터가 엄청 컸다 > 기존에는 못 바꿨지만 AQE는 바꿈
- DAG 짜고 끝나는게 아니라 실행하면서 계속 바꿔야함
- 모든 통계를 모아서 평가한 후에 RuntimeStatistics를 MapOutputStatistics 얘가 준다.
- 가장 중요한 건 끝날 때까지 끊임없이 Optimizing해야함
- Join Strategy에서 sort-merge join은 정말 답이 없을 때 함.
  
- Join Strategy : 두 개의 데이터셋에 대한 얘기
- Skewed Joins : 한 개의 데이터셋에 Skew가 일어났을때의 얘기

- When not to use AQE
  1. Job이 되게 단순하면 안 써도 됨 > 오히려 성능을 낮출 수 있음
  2. 확신이 있으니까 굳이 안 해도 됨 > AQE가 하던 일을 완벽히 이해하고 코드를 다 짜놔야함
  3. 내가 AQE보다 낫다싶으면 써라
 
- MapReduce는 iteration을 하지 않고 한 번만 하는 간단한 구조
- 한 번 돌려가지고는 결과가 안 나오기 때문에 Shuffle을 먼저 해줘야함
- 결국 중요한 건 Shuffle을 잘 이해해야된다.

### Team2 Homework
Q1) Catalyst는 실행하기 전인데 데이터 크기를 어떻게 알까?

  A) 실행 전에 정확한 실제 크기를 아는 게 아니라 추정치(estimate)를 쓴다. 이 추정치는 테이블/파일 메타데이터와 통계에서 오고, 부족하면 보수적 기본값을 쓴 뒤 실행 중(AQE) 실제 통계를 보고 계획을 갈아끼운다.
  Actual size를 보고 사이즈가 적으면 SortMergeJoin을 BroadCastHashJoin으로 바꿀 생각을 해야됨

Q2) OptimizeLocalShuffleReader란?

  A) 원래 셔플 과정에서 데이터를 여러 노드로 흩뿌리고 원격 블록을 가져와야 하는데, 이건 네트워크 비용이 크다. 
  그런데 로컬 셔플 리더를 쓰면, 같은 노드에 이미 필요한 데이터 블록이 있을 때 굳이 네트워크를 타지 않고 로컬에서 바로 읽어버림
   - SMG > BHJ 전환 시 : AQE가 BHJ로 바꿔버리면, 원래 있던 셔플 파티션이 불필요해짐, 이때 굳이 원격 셔플을 안 하고 로컬에서 읽어들이면 속도가 훨씬 빨라짐
   - 추가 셔플 파티셔닝이 필요 없을 때 : AQE가 "이 정도 파티션 수며니 충분하네"하고 더 이상 파티션을 쪼개지 않는 경우, 이때도 로컬에서만 블록을 읽게 해서 오버헤드를 줄임

- 얻는 효과
  a. Data Locality 향상 -> 네트워크 전송 최적화
  b. Shuffle Overhead 감소 -> Disk I/O + Network I/O가 줄어듬
  c. 파티션 재균형(coalesce/rebalance)과 시너지 -> 데이터가 특정 노드에만 치우쳐도, AQE가 균형잡고 로컬 리더가 받아주니까 전체적인 성능이 좋아짐

- 즉, 이미 노드 안에 필요한 데이터가 있는데, 왜 굳이 네트워크를 타고 멀리서 가져오냐 / 가까운 데서 읽자라는 최적화

## 회고
### Keep
- 

### Problem
- 아무리봐도 우리가 짠 데이터 파이프라인을 1~2주 안에 완성시킨다는건 말이 안 된다 생각을 했는데 아니나다를까 강사님께 가져갔더니 낚시를 하러갔는데 유람선을 빌려왔다라는 말을 하셨다 > 데이터 엔지니어가 할 일과 프론트/백엔드가 할 일을 구분해서 그들한테 줄 API를 만들어봤다.
- 당연한 얘기지만 오늘 강의를 들으면서 아직 Spark에 대해서 모르는게 많다는 생각이 들었고, 우리 프로젝트에 필요한 데이터를 정확하게 이해하고 최적화해야겠다는 생각을 했고, Spark에 대해 듣고 있는 강의를 최대한 빨리 들어서 프로젝트에 녹여내야겠다

### Try
- 
