## 리뷰
### 강의
- spark를 튜닝하는게 아니라 Job을 튜닝하는것이다
- data = Job 데이터를 모르고 optimizing을 한다는건 말이 안 됨 > 오늘 배우는 optimizing은 우리가 함
- the communication between nodes : spark 구조에서 제일 bottle neck임 > 그래서 shuffle을 어떻게 줄일지 혈안이다
- parquet를 쓰면 shuffle을 줄일 수 있다?
- push down
  - df.select(나이,성별).filter("age>20") >> 연산이 훨씬 크다. 
  - df.filter("age>20").select(나이,성별)
- Data Warehouse : 목적성을 갖는다
- Data Lake : 데이터를 일단 다 때려넣는다. 자주 쓰는 데이터는 structured, 로그 같은 데이터는 unstructured로 때려넣음
  - ELT가 그래서 나옴, 일단 저장해놓고 필요하면 Transform해서 씀
- Data Lakehouse : Cloud가 필요함
- network를 타는게 spark에서 제일 느린 bottle neck임
- repartitionByRange : 데이터를 보고 repartitioning 할 지를 결정해야된다. 예를 들면 서울역 일요일/ 금요일 승객이 다르니까 다르게 처리해야되듯이
- 스파크는 인메모리 툴이 아니다. LRU를 가진
- 캐시는 메모리에 들어가고, persist는 메모리나 디스크에 저장
- cache, persist, checkpoint(리니지를 갖고있는 RDD가 아니다)
- persist & checkpoint를 같이 쓰면 cache는 그 DAG안에서 두 번 세 번 쓰일때 사용, checkpoint는 다른 spark Job에서 다시 사용할 수 있으니까 같이 사용
- 결국 우리는 데이터를 보고 DAG를 개선하는 일이다.


### 최종 프로젝트 아이디어 구체화



## 회고
### Keep
- 

### Problem
-

### Try
-
