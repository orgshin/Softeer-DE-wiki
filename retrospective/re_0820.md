## 리뷰

### 오늘 한 일
- EC2와 S3를 연결하고, 버킷(team2-html-raw, team2-html-out)을 정상적으로 생성했다. IAM Role을 만들고 ec2와 s3를 연결하려고 했는데, ec2에서 role을 부여하려고 했더니 목록에 뜨지 않아 댕글님한테 권한을 받으면서 AWS 교육을 다시 받게 되었다.
- AWS CLI로 접근 권한을 점검하면서 IAM Role과 EC2 권한 연계를 학습했다.
  - EC2 인스턴스 접속 및 기본 설정 완료
  - 데이터 파이프라인의 각 단계(Fetcher, Parser)와 S3 버킷(raw, out)의 역할 정의
 
### 오늘 집에 가서 해야할 것
- partsro_fetcher.py를 완성해서 Airflow DAG과 실제로 연결해보기
- Parser를 스파크 코드로 바꾸고, Spark 파서 실행을 통해 S3(raw → out) 데이터 흐름을 검증하기
  
## 회고
### Keep
- 

### Problem
- DAG 경로가 비어 있는 문제가 있었다. 즉, 아직 실제 DAG 코드가 컨테이너 내부로 안 들어간 상태라서, Airflow 웹 UI에서 아무것도 뜨지 않아 살짝 문제를 겪었는데 환경변수 설정을 통해 해결했다.
- airflow와 rds에 대해 공부를 좀 해와서 내일은 프로토타입을 완성하고 피드백을 받는게 목표다.

### Try
- 
