## 리뷰

### 데일리 스크럼

### 과제 수행을 위해 해야 할 일 (간략히)
- 웹 스크래핑을 통해 IMF에서 제공하는 국가별 GDP를 높은 순서대로 테이블 형태로 출력
     - 매년 자료가 갱신되더라도 해당 코드를 재사용해서 정보를 얻을 수 있어야 함
     - GDP가 100B USD이상이 되는 국가만 화면에 출력
     - 각 Region별로 Top5 국가의 GDP 평균을 구해 화면에 출력
       
     - Extract -> BeautifulSoup4 
     - Transform -> Python, Pandas
     - Load -> load_to_json, SQLite3 Library(DB 생성)

### 오늘 한 일
  - 텍스트 형태의 데이터에서 원하는 HTML 요소에 어떻게 접근할 수 있을까?
    - BeautifulSoup : 날 것의 HTML을 의미있는 객체로 만들어서 사용자가 원하는 데이터를 가져올 수 있게 만들어줌
  - ETL Process
    - E(Extract) : Wikipedia의 GDP 데이터를 웹 스크래핑 ( 웹 스크래핑 라이브러리에 대해 공부함 )
      - extract_region_data 함수 : 각 지역별 GDP 데이터를 해당 wikipedia page에서 가져옴
    - T(Transform) : 문자열 처리, 단위 환산(GDP 단위 변환), 필터링 및 정렬
    - L(Load) : 1차로 JSON 저장( Countries_by_GDP.json ), 2차로 SQLite DB 저장( World_Economies.db )
      - load_to_json 함수 : 가공된 데이터를 JSON 파일로 저장
      - load_to_db 함수 : 가공된 데이터를 SQLite DB에 저장
    - Log system을 추가해 각 단계의 시작과 끝을 기록

       - Problem : Wikipedia URL로 웹 스크래핑을 했기때문에 Main GDP 테이블만 이용가능 > Region 데이터를 이용할 수 없어, 각 Region별로 Top5 국가의 GDP 평균을 구할 수 없음
       - Solving : 지역별 페이지 URL 리스트를 정의해 각 Region Table을 이용가능하도록 만들었음
     
    - 팀 활동 요구사항
      - IMF는 주로 IMF Data 포털을 통해 데이터를 제공 -> 데이터를 다운받거나 API를 통해 접근할 수 있음
        1. API 문서 확인 : 데이터셋의 엔드포인트, 파라미터, 응답 형식 등을 파악
        2. 'Requests' 라이브러리 사용 : API 엔드포인트에 HTTP GET 요청을 보냄
        3. 응답처리 : JSON or XML 형식으로 제공, response.json() or Beautiful(XML의 경우)를 사용하여 응답을 파싱하고 pandas.DataFrame으로 변환
           - 장점 : 공식 소스에서 가져오는 데이터라 신뢰성 높음, 데이터가 업데이트되는 즉시 접근 가능, API를 사용하면 자동화가 가능, 조건에 따른 필터링 가능
           - 단점 : IMF API 사용량 제한, 응답 구조가 더 복잡할 수 있음

       - 과거 데이터에 대한 조회가 필요하다면, 현재 짠 코드의 if_exists='replace' 방식 대신 snapshot_date 컬럼을 추가하고 증분 로드 전략( Upsert or 스냅샷 테이블 방식 )을 사용하여 ETL Process를 변경해야함 -> 이렇게 하면 데이터 이력을 보존하고, 필요에 따라 과거 데이터를 조회할 수 있게 됨


