## 리뷰

### 데일리 스크럼

### 과제 수행을 위해 해야 할 일 (간략히)
- 웹 스크래핑을 통해 IMF에서 제공하는 국가별 GDP를 높은 순서대로 테이블 형태로 출력
     - 매년 자료가 갱신되더라도 해당 코드를 재사용해서 정보를 얻을 수 있어야 함
     - GDP가 100B USD이상이 되는 국가만 화면에 출력
     - 각 Region별로 Top5 국가의 GDP 평균을 구해 화면에 출력
       
     - Extract -> BeautifulSoup4 
     - Transform -> Python, Pandas
     - Load -> load_to_json, SQLite3 Library(DB 생성)

### 오늘 한 일
  - 텍스트 형태의 데이터에서 원하는 HTML 요소에 어떻게 접근할 수 있을까?
    - BeautifulSoup : 날 것의 HTML을 의미있는 객체로 만들어서 사용자가 원하는 데이터를 가져올 수 있게 만들어줌
  - ETL Process
    - E(Extract) : Wikipedia의 GDP 데이터를 웹 스크래핑 ( 웹 스크래핑 라이브러리에 대해 공부함 )
      - extract_region_data 함수 : 각 지역별 GDP 데이터를 해당 wikipedia page에서 가져옴
    - T(Transform) : 문자열 처리, 단위 환산(GDP 단위 변환), 필터링 및 정렬
    - L(Load) : 1차로 JSON 저장( Countries_by_GDP.json ), 2차로 SQLite DB 저장( World_Economies.db )
      - load_to_json 함수 : 가공된 데이터를 JSON 파일로 저장
      - load_to_db 함수 : 가공된 데이터를 SQLite DB에 저장
    - Log system을 추가해 각 단계의 시작과 끝을 기록

       - Problem : Wikipedia URL로 웹 스크래핑을 했기때문에 Main GDP 테이블만 이용가능 > Region 데이터를 이용할 수 없어, 각 Region별로 Top5 국가의 GDP 평균을 구할 수 없음
       - Solving : 지역별 페이지 URL 리스트를 정의해 각 Region Table을 이용가능하도록 만들었음
     
     - 팀 활동 요구사항
      - IMF는 주로 IMF Data 포털을 통해 데이터를 제공 -> 데이터를 다운받거나 API를 통해 접근할 수 있음
        1. API 문서 확인 : 데이터셋의 엔드포인트, 파라미터, 응답 형식 등을 파악
        2. 'Requests' 라이브러리 사용 : API 엔드포인트에 HTTP GET 요청을 보냄
        3. 응답처리 : JSON or XML 형식으로 제공, response.json() or Beautiful(XML의 경우)를 사용하여 응답을 파싱하고 pandas.DataFrame으로 변환
           - 장점 : 공식 소스에서 가져오는 데이터라 신뢰성 높음, 데이터가 업데이트되는 즉시 접근 가능, API를 사용하면 자동화가 가능, 조건에 따른 필터링 가능
           - 단점 : IMF API 사용량 제한, 응답 구조가 더 복잡할 수 있음

       - 과거 데이터에 대한 조회가 필요하다면, 현재 짠 코드의 if_exists='replace' 방식 대신 snapshot_date 컬럼을 추가하고 증분 로드 전략( 스냅샷 테이블 방식 or SCD Type 2 )을 사용하여 ETL Process를 변경해야함 -> 이렇게 하면 데이터 이력을 보존하고, 필요에 따라 과거 데이터를 조회할 수 있게 됨
       -  snapshot_date 컬럼 및 증분 로드 전략은 "데이터 웨어하우스나 분석시스템"에서 데이터 이력을 관리하는 데 매우 중요한 개념
1. 'snapshot_date'( Append Only ): 특정 시점에 추출되거나 기록된 데이터의 "버전"을 식별하는 데 사용되는 타임스탬프 또는 날짜 컬럼
   - 데이터 이력 추적 : 데이터가 시간이 지남에 따라 어떻게 변했는지 추적할 수 있게 함
   - 특정 시점의 데이터 조회 : 과거 특정 시점의 데이터를 "그때 그 모습 그대로" 조회할 수 있게 함 > 시계열 분석에 필수적

2. 증분 로드( Incremental Load ) 전략 : 소스 시스템에서 변경되거나 새로 생성된 데이터만 추출하여 대상 시스템에 로드하는 ETL 전략, 전체 데이터를 매번 로드하는 전체 로드( Full load ) 방식과 대조
   - 효율성 : 매번 전체 데이터를 처리할 필요가 없어 ETL 작업 시간을 단축하고 시스템 리소스 사용량을 줄임
   - 성능 : 데이터 양이 많을수록 증분 로드의 성능 이점이 커짐
   - 데이터 이력 보존 : 과거 데이터를 유지하면서 최신 데이터를 통합할 수 있어, 시계열 분석이나 감사 요구사항을 충족시킬 수 있음
  
   - But, 데이터 베이스의 크기가 증가하고 복잡성이 증가해 조회하기 어려울 수 있음
  
   - Type 2 Slowly Changing Dimensionns( SCD Type 2 ) : 레코드의 변경 이력을 완전히 보존하면서도 특정 시점의 유효한 데이터를 쉽게 조회할 수 있도록 함
      - 완전한 이력 보존 : 모든 데이터 변경 이력을 정확하게 추적할 수 있음
      - 특정 시점 조회 용이 : start_date, end_date가 있어 특정 시점의 데이터를 쉽게 조회할 수 있음
    
      - But, 구현 복잡성 증가 및 데이터베이스의 크기가 증가할 수 있는 단점이 존재
     
   - Upsert(Updat/Insert) :대상 테이블에 이미 존재하는 레코드인 경우 해당 레코드를 업데이트하고, 존재하지 않는 새로운 레코드인 경우 삽입하는 전략 "Update if exists, Insert if not exists"
      - 항상 최신 상태의 데이터만 필요한 경우
      - 데이터베이스 크기 관리가 중요한 경우에 사용


