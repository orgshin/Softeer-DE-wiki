## 리뷰

### 강의
- 숙제가 거짓말이었다. 인-메모리라고 말하는 순간 틀렸다. 스파크가 어떻게 일어나는지 몰라서 일어난 착각.
- 스토리지 메모리를 기본적으로 안 씀. execution memory가 꽉 차면 storage memory로 밀려 올라감. storage memory는 cache(LRU)의 땅. checkpoint는 로컬에 저장
- 최소의 비용으로 최대의 효율을 내는건 말이 안 됨 >> 찾아보기
- shuffle은 stage 끝에 일어남.
- Execution memory에서 공간이 없으면 치고 올라가서 Storage memory가 날아가면 다시 처음부터 하면서 복구하면 되니까 execution이 storage를 이기게 해놨다. 밀어내면 LRU cache에 있는 내용 중 잘 안 쓰는 필요없는걸 차례대로 없앤다.
- MEMORY_ONLY, MEMORY_AND_DISK( 사실 캐시를 했는데 디스크에 쓰는거다 > 속도가 느려짐 )는 때에따라 더 나은 걸 선택해야됨
  - 메모리는 없지만 cache는 되어있다 >> 너 나가해서 메모리가 없어짐(MEMORY_ONLY), DISK로 밀려남(MEMORY_AND_DISK)
- shuffle이 일어난 후의 데이터를 캐시하는게 낫지, 그 전에걸 캐시해서 다시 셔플하는건 비용이 많이 들고 쓸모없다.
- unpersist하면 메모리가 지워진다. 다시 쓸 생각이 없으면 unpersist하는게 맞는데 스파크가 잘 돌아가서 잘 안 씀
- 캐시를 했는데 캐시가 안 되는 상황이 있음, 스파크는 파티션을 cpu 코어 개수에 맞게 알아서 만들어주는데, spark.sql.files.maxPartitionBytes: 이걸 쓰면 더 늘어남
- 파티션의 사이즈를 줄여야되니까, 파티션 개수를 늘려야한다  혹은 셔플이 안 일어나는 join을 써야함/ 스파크 자체의 메모리를 어떻게 하는건 한계가 있다.
- 결국 우리가 하는건 파티션 조절 / 데이터를 보고 파티셔닝을 잘 해야됨
- 분산 환경에서 skewing이 제일 안 좋음
- 아무생각 없이 캐싱하는거 > DAG를 그려봤더니 여기 하면 좋을거같아 >> 신경쓰자

## 현직자 밋업

### 회사와 미션
- **미션**: 고객이 만족하는 탁월한 IT서비스 제공
- **핵심 가치**: 기능, 안정성, 성능, 보안, 민첩성
- **일하는 방식**:
  - 긍정적으로(소통·협업)
  - 책임감 있게(집요함, 전문성)
  - 대담하게(고객 최우선, 민첩한 실행, 도전)
- ICT 거버넌스: 제조, 글로벌 생산·판매, 국내·글로벌 사업, 마케팅 지원
- 생성형 AI 프롬프트 노출 제약 존재

### 직무 개요
- **정의**: 데이터 수집 → 정제 → 가공 → 추출
- **파이프라인 구조**: Source → ODS(원천 데이터) → DW → DM → Dashboard
- **데이터 소스**:
  - SAP, RDB, API, File 등
  - 해외: 보안 문제로 Kafka 거쳐 적재 후 HDFS로
  - 국내: Sqoop, Python으로 HDFS 적재
- **팀 업무**:
  - Kafka에 들어온 차량 데이터를 정리
  - 개인정보 가명화 후 제공
- **배치 처리**: Airflow 전담, Spark·Scala 공통 사용
- **추가 기술**: Trino, Tableau, Data Hub
- **역량**: 통합적 사고력, 시각화부터 플랫폼까지 전반 다룸

### 업무 환경
- 재택 1회 또는 주 3회 재택 + 자율 좌석제
- 코어타임 준수
- 매일 데일리 스크럼
- AWS + 온프레미스 혼합 환경
- DW/DM 데이터 수집 → 통합 플랫폼 전환 중 (Iceberg 도입 검토)
- 도메인 지식은 입사 후 습득 가능
- 문서화 역량 필수

### 협업과 역할
- CDO 내 데이터 플랫폼팀, 데이터 서비스팀과 협업
- 데이터 사이언티스트·원천 DB 담당자와도 협업 (개인정보 관리 중요)
- 요청받은 데이터를 개인 단위로 처리하는 경우 많음
- 적극성, 빠른 학습력, 오픈소스 도입 능력, 기본 전공지식 필수

### 개선·자동화 방향
- 현재: 고객센터처럼 요청 데이터 제공
- 목표: 자동화 + UI 플랫폼화
- 데이터 리니지 관리
- 대량 테이블 자동 수집·제공
- 알람 시스템 구축 (배치 실패 / 데이터 건수 불일치 감지)
- 서버·노드 성능 문제 대응 필요

### 툴·기술
- PyCharm, Kubernetes(k8s, OpenLens) 권장
- GitLab 사용, 코드 리뷰·크로스 체크 필수
- 데이터옵스
- 증분·풀 적재 방식 운영
- AWS 비용 절감:
  - 배치 시간대만 서버 기동
  - IaC로 인프라 관리

### 면접·조언
- “개발 흥미 없다” 같은 표현 금지 → 열정·학습 의지 강조
- 기술 선택 이유, 대안 기술 미사용 이유 준비
- LLM은 정확도 90% 이상 아니면 서비스보단 인프라 고도화 우선
- 어떤 클라우드든 빠르게 전환 가능 역량 중요
- 현대차·기아차에서 대량 데이터 다루고 싶다는 구체적 포부 제시


### 최종 프로젝트

## 회고
### Keep
- 

### Problem
- 데이터 파이프라인 설계를 하는데 어려움을 겪고 있다. 연휴때 aws services와 airflow에 대해 공부를 해보려고합니다.

### Try
- 
